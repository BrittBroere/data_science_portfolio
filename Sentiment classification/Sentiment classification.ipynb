{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "guided-withdrawal",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This is a sentiment classification task using Doc2Vec in Python. Sentiment analysis is about extracting a set of opinion representations from a text. Each representation should identify the opinion holder, target, content, and context. For this project, a data set containing movie reviews is used. It is assumed that most of the elements in an opinion representation are already known, so the goal of the project is to classify these. Sentiment classification takes as input an opinionated text object and the output is typically a sentiment label. This sentiment label can either be a category such as positive, negative or neutral (polarity analysis) or it characterizes the feeling of the person who generated the text (emotion analysis).\n",
    "\n",
    "### Method\n",
    "The method entails of loading the data, pre-processing it into the desired structure, building and training the model, classifying sentiments and experimenting with different settings. \n",
    "\n",
    "### Data \n",
    "This data set is about movie reviews along with their binary sentiment polarity labels. It contains 50,000 reviews split evenly into 25,000 train and 25,000 test sets. Each set has 12,500 negative and 12,500 positive labels. A review is labeled as negative when it has a score of four or less out of 10. On the other hand, a review is labeled as positive when it has a score of seven or higher out of 10. However, reviews with scores inbetween and thus more neutral reviews are not included.\n",
    "\n",
    "### Setup\n",
    "In Python, gensim is used for the implementation of Doc2Vec. Before inputting the data into the model, the data has to be\n",
    "pre-processed. The data is being cleaned by converting everything into lower case and removing punctuation. The result is four documents: two test files, one containing positive movie reviews and one containing negative movie reviews and two train files, one containing positive movie reviews and the other containing negative movie reviews. \n",
    "\n",
    "Each review should be on one entire line, separated by new lines. Also, the precision, recall and F1 score are reported as evaluation metrics. Precision is defined as the proportion of assigned labels that are correct. Recall is defined as the proportion of true positives that were identified correctly. The F1-score is a measure that considers aspects of both precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-hours",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "Let's start by importing the modules that are required for this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dynamic-shift",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "import gensim.models.doc2vec\n",
    "assert gensim.models.doc2vec.FAST_VERSION \n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import smart_open\n",
    "import numpy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "configured-melbourne",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "my_tar = tarfile.open('aclImdb_v1.tar.gz')\n",
    "my_tar.extractall() # specify which folder to extract to\n",
    "my_tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-display",
   "metadata": {},
   "source": [
    "# 1. Data loading\n",
    "In this step, the data is loaded. Read the positive and negative examples and stores them as training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "vietnamese-protocol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train positives\n",
    "reviews_train_pos = []\n",
    "files_train_pos = os.listdir('aclImdb/train/pos')\n",
    "for i in files_train_pos:\n",
    "    with open('aclImdb/train/pos/' + i, 'r', encoding = 'utf-8') as file: \n",
    "        data = file.read()\n",
    "        out = re.sub(r'[^a-zA-Z0-9\\s]', '', data).lower()\n",
    "    reviews_train_pos.append(out)\n",
    "    \n",
    "# train negatives \n",
    "reviews_train_neg = []\n",
    "files_train_neg = os.listdir('aclImdb/train/neg')\n",
    "for i in files_train_neg:\n",
    "    with open('aclImdb/train/neg/' + i, 'r', encoding = 'utf-8') as file: \n",
    "        data = file.read()\n",
    "        out = re.sub(r'[^a-zA-Z0-9\\s]', '', data).lower()\n",
    "    reviews_train_neg.append(out)\n",
    "    \n",
    "# test positives\n",
    "reviews_test_pos = []\n",
    "files_test_pos = os.listdir('aclImdb/test/pos')\n",
    "for i in files_test_pos:\n",
    "    with open('aclImdb/test/pos/' + i, 'r', encoding = 'utf-8') as file: \n",
    "        data = file.read()\n",
    "        out = re.sub(r'[^a-zA-Z0-9\\s]', '', data).lower()\n",
    "    reviews_test_pos.append(out)\n",
    "    \n",
    "# test negatives\n",
    "reviews_test_neg = []\n",
    "files_test_neg = os.listdir('aclImdb/test/neg')\n",
    "for i in files_test_neg:\n",
    "    with open('aclImdb/test/neg/' + i, 'r', encoding = 'utf-8') as file: \n",
    "        data = file.read()\n",
    "        out = re.sub(r'[^a-zA-Z0-9\\s]', '', data).lower()\n",
    "    reviews_test_neg.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "informational-welding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test negatives\n",
    "with open('test-neg.txt', 'w',encoding='utf-8') as f: \n",
    "    for i in reviews_test_neg:\n",
    "        f.write(i+'\\n') \n",
    "        \n",
    "# test positives\n",
    "with open('test-pos.txt', 'w',encoding='utf-8') as f: \n",
    "    for i in reviews_test_pos:\n",
    "        f.write(i+'\\n') \n",
    "        \n",
    "# train negatives \n",
    "with open('train-neg.txt', 'w',encoding='utf-8') as f: \n",
    "    for i in reviews_train_neg:\n",
    "        f.write(i+'\\n') \n",
    "        \n",
    "# train positives\n",
    "with open('train-pos.txt', 'w',encoding='utf-8') as f: \n",
    "    for i in reviews_train_pos:\n",
    "        f.write(i+'\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-determination",
   "metadata": {},
   "source": [
    "This is what a positive movie review looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "offensive-likelihood",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my 35 years in the teaching profession lead me to believe that bromwell highs satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled  at  high a classic line inspector im here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isnt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_train_pos[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-marble",
   "metadata": {},
   "source": [
    "# 2. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-brazil",
   "metadata": {},
   "source": [
    "## 2.1 Baseline model\n",
    "\n",
    "### 2.1.1 Doc2Vec\n",
    "Doc2Vec converts a word into a vector and also aggregates all the words in a sentence into a vector. It treats a sentence label as a special word and does something with that special word.\n",
    "So sentences have to be formatted into:\n",
    "\n",
    "$$[['word1', 'word2', 'word3', 'lastword'], ['label1']]$$\n",
    "\n",
    "LabeledSentence, a class from gensim.models.doc2vec, is a way to do that. It contains a list of words, and a label for the sentence. However, LabeledSentence can do that for a single text file, but not for multiple files. That is why the LabeledLineSentence class is written. The constructor takes in a dictionary that defines the files to read and the label prefixes sentences from that document should take on. Then, Doc2Vec can either read the collection directly via\n",
    "the iterator, or the array can be accessed directly. This class also has a function to return a permutated version of the array of LabeledSentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "composite-tiffany",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "        \n",
    "        flipped = {}\n",
    "        \n",
    "        # make sure that keys are unique\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])\n",
    "    \n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with smart_open.open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append(LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "    \n",
    "    def sentences_perm(self):\n",
    "        shuffled = list(self.sentences)\n",
    "        random.shuffle(shuffled)\n",
    "        return shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-conspiracy",
   "metadata": {},
   "source": [
    "Doc2Vec requires a vocabulary table, so model.build\\_vocab is used which takes an array of LabeledLineSentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "similar-insight",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = {'test-neg.txt':'TEST_NEG', 'test-pos.txt':'TEST_POS', 'train-neg.txt':'TRAIN_NEG', 'train-pos.txt':'TRAIN_POS'}\n",
    "\n",
    "sentences = LabeledLineSentence(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-compatibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=7)\n",
    "\n",
    "model.build_vocab(sentences.to_array())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-investigation",
   "metadata": {},
   "source": [
    "A short description of the parameters:\n",
    "* min_count: ignore all words with total frequency lower than this. This has to be set to 1, since the sentence labels only appear once. Setting it any higher than 1 will miss out on the sentences.\n",
    "* window: the maximum distance between the current and predicted word within a sentence. Word2Vec uses a skip-gram model, and this is simply the window size of the skip-gram model.\n",
    "* size: dimensionality of the feature vectors in output. 100 is a good number. \n",
    "* sample: threshold for configuring which higher-frequency words are randomly downsampled\n",
    "* workers: use this many worker threads to train the model\n",
    "\n",
    "After building the model, the model has to be trained. The model is trained for 25 epochs. In each training epoch, the sequence of sentences fed to the model is randomized because then the model is trained better. The function sentences\\_perm from the LabeledLineSentences class does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-level",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(25):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(sentences.sentences_perm(),\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./imdb.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dramatic-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load('./imdb.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mental-think",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('decent', 0.7449836134910583),\n",
       " ('great', 0.737765908241272),\n",
       " ('nice', 0.7261615991592407),\n",
       " ('bad', 0.7239553332328796),\n",
       " ('fine', 0.6863929033279419),\n",
       " ('excellent', 0.6742612719535828),\n",
       " ('solid', 0.6639758348464966),\n",
       " ('terrific', 0.6231818795204163),\n",
       " ('fantastic', 0.6168004870414734),\n",
       " ('wonderful', 0.611182451248169)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "handmade-fever",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dull', 0.7971225380897522),\n",
       " ('tedious', 0.7050924301147461),\n",
       " ('pointless', 0.6939346194267273),\n",
       " ('uninteresting', 0.6464412212371826),\n",
       " ('predictable', 0.621228814125061),\n",
       " ('unoriginal', 0.584164023399353),\n",
       " ('bored', 0.5438627004623413),\n",
       " ('annoying', 0.5417511463165283),\n",
       " ('stupid', 0.537611722946167),\n",
       " ('ridiculous', 0.5341532230377197)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('boring')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-wrong",
   "metadata": {},
   "source": [
    "Now that the model has converted each sentence into a vector, these vectors are used to train a classifier. A train\\_array is created where the first 12,500 rows contain the positive review vectors and the second 12,500 rows contain the negative review vectors. There is another array,\n",
    "called train\\_labels of which the first 12,500 rows have the value one (corresponding to the positive reviews) and the other 12,500 rows have the value zero (corresponding to the negative reviews). The same is done for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "purple-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arrays = numpy.zeros((25000, 100))\n",
    "train_labels = numpy.zeros(25000, dtype=int)\n",
    "\n",
    "for i in range(12500):\n",
    "    prefix_train_pos = 'TRAIN_POS_' + str(i)\n",
    "    prefix_train_neg = 'TRAIN_NEG_' + str(i)\n",
    "    train_arrays[i] = model[prefix_train_pos]\n",
    "    train_arrays[12500 + i] = model[prefix_train_neg]\n",
    "    train_labels[i] = 1\n",
    "    train_labels[12500 + i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "behind-platform",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arrays = numpy.zeros((25000, 100))\n",
    "test_labels = numpy.zeros(25000, dtype=int)\n",
    "\n",
    "for i in range(12500):\n",
    "    prefix_test_pos = 'TEST_POS_' + str(i)\n",
    "    prefix_test_neg = 'TEST_NEG_' + str(i)\n",
    "    test_arrays[i] = model[prefix_test_pos]\n",
    "    test_arrays[12500 + i] = model[prefix_test_neg]\n",
    "    test_labels[i] = 1\n",
    "    test_labels[12500 + i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-canal",
   "metadata": {},
   "source": [
    "Now, a logistic regression classifier is trained on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "executive-nevada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_arrays, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "enhanced-legislature",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.872"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy\n",
    "classifier.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "included-graduation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.87319   0.87040   0.87179     12500\n",
      "           1    0.87081   0.87360   0.87220     12500\n",
      "\n",
      "    accuracy                        0.87200     25000\n",
      "   macro avg    0.87200   0.87200   0.87200     25000\n",
      "weighted avg    0.87200   0.87200   0.87200     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict = classifier.predict(test_arrays)\n",
    "print(metrics.classification_report(test_labels, predict, digits = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "horizontal-german",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = metrics.classification_report(test_labels, predict, digits = 5, output_dict = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-operation",
   "metadata": {},
   "source": [
    "The accuracy of the classifier is about 87% for sentiment analysis. This is quite good, given that only a linear SVM and a very shallow neural network are used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-omega",
   "metadata": {},
   "source": [
    "## 2.2 Experimental setting 1\n",
    "Experimental setting 1 is similar to the baseline setting but the way the reviews are pre-processed is changed. Still, everything is converted into lower case and the punctuation is removed but now also the reviews are lemmatized. A lemma is the dictionary form of a word.\n",
    "\n",
    "###### Lemmatize the review files and store them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "recorded-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemmatized_text(corpus):\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) for review in corpus]\n",
    "\n",
    "lemmatized_train_pos = get_lemmatized_text(reviews_train_pos)\n",
    "lemmatized_train_neg = get_lemmatized_text(reviews_train_neg)\n",
    "lemmatized_test_pos = get_lemmatized_text(reviews_test_pos)\n",
    "lemmatized_test_neg = get_lemmatized_text(reviews_test_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "complicated-treasury",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test negatives\n",
    "with open('lemmatized_test-neg.txt', 'w',encoding='utf-8') as f: \n",
    "    for i in lemmatized_test_neg:\n",
    "        f.write(i+'\\n') \n",
    "        \n",
    "# test positives\n",
    "with open('lemmatized_test-pos.txt', 'w',encoding='utf-8') as f: \n",
    "    for i in lemmatized_test_pos:\n",
    "        f.write(i+'\\n') \n",
    "        \n",
    "# train negatives\n",
    "with open('lemmatized_train-neg.txt', 'w',encoding='utf-8') as f: \n",
    "    for i in lemmatized_train_neg:\n",
    "        f.write(i+'\\n') \n",
    "\n",
    "# train positives\n",
    "with open('lemmatized_train-pos.txt', 'w',encoding='utf-8') as f: \n",
    "    for i in lemmatized_train_pos:\n",
    "        f.write(i+'\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-thickness",
   "metadata": {},
   "source": [
    "### 2.2.1 Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "spectacular-isolation",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources1 = {'lemmatized_test-neg.txt':'LEM_TEST_NEG', 'lemmatized_test-pos.txt':'LEM_TEST_POS',\n",
    "           'lemmatized_train-neg.txt':'LEM_TRAIN_NEG', 'lemmatized_train-pos.txt':'LEM_TRAIN_POS'}\n",
    "\n",
    "sentences1 = LabeledLineSentence(sources1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-disabled",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=7)\n",
    "\n",
    "model1.build_vocab(sentences1.to_array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-detail",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(25):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model1.train(sentences1.sentences_perm(),\n",
    "                total_examples=model1.corpus_count,\n",
    "                epochs=model1.iter,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-mining",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save('./imdb.d2v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "nearby-species",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Doc2Vec.load('./imdb.d2v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "sudden-arrow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('decent', 0.7839062213897705),\n",
       " ('great', 0.7348613142967224),\n",
       " ('bad', 0.6999446749687195),\n",
       " ('solid', 0.6808467507362366),\n",
       " ('nice', 0.6562058329582214),\n",
       " ('fine', 0.6537462472915649),\n",
       " ('poor', 0.6355423927307129),\n",
       " ('excellent', 0.6106948852539062),\n",
       " ('terrible', 0.6053005456924438),\n",
       " ('terrific', 0.6030822992324829)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.most_similar('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "minimal-services",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dull', 0.7662643194198608),\n",
       " ('pointless', 0.6455971002578735),\n",
       " ('tedious', 0.6319400668144226),\n",
       " ('predictable', 0.6287767887115479),\n",
       " ('unfunny', 0.5826073884963989),\n",
       " ('uninteresting', 0.5795999765396118),\n",
       " ('annoying', 0.5657222270965576),\n",
       " ('bored', 0.5572026968002319),\n",
       " ('stupid', 0.5556310415267944),\n",
       " ('lame', 0.5352811813354492)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.most_similar('boring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "private-certificate",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arrays1 = numpy.zeros((25000, 100))\n",
    "train_labels1 = numpy.zeros(25000, dtype=int)\n",
    "\n",
    "for i in range(12500):\n",
    "    prefix_train_pos1 = 'LEM_TRAIN_POS_' + str(i)\n",
    "    prefix_train_neg1 = 'LEM_TRAIN_NEG_' + str(i)\n",
    "    train_arrays1[i] = model1[prefix_train_pos1]\n",
    "    train_arrays1[12500 + i] = model1[prefix_train_neg1]\n",
    "    train_labels1[i] = 1\n",
    "    train_labels1[12500 + i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "authorized-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arrays1 = numpy.zeros((25000, 100))\n",
    "test_labels1 = numpy.zeros(25000, dtype=int)\n",
    "\n",
    "for i in range(12500):\n",
    "    prefix_test_pos1 = 'LEM_TEST_POS_' + str(i)\n",
    "    prefix_test_neg1 = 'LEM_TEST_NEG_' + str(i)\n",
    "    test_arrays1[i] = model1[prefix_test_pos1]\n",
    "    test_arrays1[12500 + i] = model1[prefix_test_neg1]\n",
    "    test_labels1[i] = 1\n",
    "    test_labels1[12500 + i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "peripheral-compiler",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier1 = LogisticRegression()\n",
    "classifier1.fit(train_arrays1, train_labels1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "governing-chancellor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87108"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy\n",
    "classifier1.score(test_arrays1, test_labels1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "authorized-navigator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.87188   0.87000   0.87094     12500\n",
      "           1    0.87028   0.87216   0.87122     12500\n",
      "\n",
      "    accuracy                        0.87108     25000\n",
      "   macro avg    0.87108   0.87108   0.87108     25000\n",
      "weighted avg    0.87108   0.87108   0.87108     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict = classifier1.predict(test_arrays1)\n",
    "print(metrics.classification_report(test_labels1, predict, digits = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "heavy-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict1 = metrics.classification_report(test_labels1, predict, digits = 5, output_dict = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-advancement",
   "metadata": {},
   "source": [
    "## 2.3 Experimental setting 2\n",
    "For experimental setting 2, the same classifier is used but this classifier is tuned. So the classifier is still logistic regression like in the baseline setting, but now the penalty is changed from L2 regularization to L1 regularization. In addition, the parameter C (the inverse of the regularization strength) is set to 0.25 instead of 1.0. A smaller value for C specifies stronger regularization.\n",
    "\n",
    "###### Tune the classifier, Doc2Vec procedure is the same as in the baseline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "emotional-sperm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.25, penalty='l1', solver='saga')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier2 = LogisticRegression(C=0.25, penalty = 'l1', solver = 'saga')\n",
    "classifier2.fit(train_arrays, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "pacific-headline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8718"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy\n",
    "classifier2.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "intelligent-mouth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.87290   0.87032   0.87161     12500\n",
      "           1    0.87070   0.87328   0.87199     12500\n",
      "\n",
      "    accuracy                        0.87180     25000\n",
      "   macro avg    0.87180   0.87180   0.87180     25000\n",
      "weighted avg    0.87180   0.87180   0.87180     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict = classifier2.predict(test_arrays)\n",
    "print(metrics.classification_report(test_labels, predict, digits = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "streaming-failing",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict2 = metrics.classification_report(test_labels, predict, digits = 5, output_dict = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-rubber",
   "metadata": {},
   "source": [
    "## 2.4 Experimental setting 3\n",
    "Experimental setting 3 has a change in the word embeddings model. The parameter size of the word embeddings model in the baseline setting is set to 100. This means that model outputs the review text as a 100 dimensional vector. The feature vector space of the word embeddings model in the third experimental setting is 300 instead of 100 as in the baseline setting.\n",
    "\n",
    "###### The size parameter in Doc2Vec is increased from 100 to 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-algeria",
   "metadata": {},
   "source": [
    "### 2.4.1 Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-medium",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Doc2Vec(min_count=1, window=10, size=300, sample=1e-4, negative=5, workers=7)\n",
    "\n",
    "model2.build_vocab(sentences.to_array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-mailman",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(25):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model2.train(sentences.sentences_perm(),\n",
    "                total_examples=model2.corpus_count,\n",
    "                epochs=model2.iter,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-discretion",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('./imdb.d2v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "intimate-province",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Doc2Vec.load('./imdb.d2v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "modular-elimination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('great', 0.6714057922363281),\n",
       " ('bad', 0.6277437210083008),\n",
       " ('decent', 0.5501477122306824),\n",
       " ('for', 0.534475564956665),\n",
       " ('but', 0.5228431224822998),\n",
       " ('the', 0.5226880311965942),\n",
       " ('that', 0.5085980892181396),\n",
       " ('nice', 0.4987344741821289),\n",
       " ('horrible', 0.4985167384147644),\n",
       " ('a', 0.49822670221328735)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.most_similar('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "gentle-husband",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dull', 0.5229418277740479),\n",
       " ('bored', 0.41893628239631653),\n",
       " ('tedious', 0.4082891047000885),\n",
       " ('it', 0.3790885806083679),\n",
       " ('stupid', 0.36691418290138245),\n",
       " ('uninteresting', 0.3656439483165741),\n",
       " ('predictable', 0.3643425703048706),\n",
       " ('boringbr', 0.3641272485256195),\n",
       " ('cheesy', 0.35982877016067505),\n",
       " ('pointless', 0.3583638072013855)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.most_similar('boring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "catholic-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arrays2 = numpy.zeros((25000, 300))\n",
    "train_labels2 = numpy.zeros(25000, dtype=int)\n",
    "\n",
    "for i in range(12500):\n",
    "    prefix_train_pos2 = 'TRAIN_POS_' + str(i)\n",
    "    prefix_train_neg2 = 'TRAIN_NEG_' + str(i)\n",
    "    train_arrays2[i] = model2[prefix_train_pos2]\n",
    "    train_arrays2[12500 + i] = model2[prefix_train_neg2]\n",
    "    train_labels2[i] = 1\n",
    "    train_labels2[12500 + i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "adjacent-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arrays2 = numpy.zeros((25000, 300))\n",
    "test_labels2 = numpy.zeros(25000, dtype=int)\n",
    "\n",
    "for i in range(12500):\n",
    "    prefix_test_pos2 = 'TEST_POS_' + str(i)\n",
    "    prefix_test_neg2 = 'TEST_NEG_' + str(i)\n",
    "    test_arrays2[i] = model2[prefix_test_pos2]\n",
    "    test_arrays2[12500 + i] = model2[prefix_test_neg2]\n",
    "    test_labels2[i] = 1\n",
    "    test_labels2[12500 + i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "tested-pathology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.5)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier3 = LogisticRegression(C = 0.5)\n",
    "classifier3.fit(train_arrays2, train_labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "quiet-pakistan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87396"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy\n",
    "classifier3.score(test_arrays2, test_labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "casual-edgar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.87616   0.87104   0.87359     12500\n",
      "           1    0.87179   0.87688   0.87433     12500\n",
      "\n",
      "    accuracy                        0.87396     25000\n",
      "   macro avg    0.87397   0.87396   0.87396     25000\n",
      "weighted avg    0.87397   0.87396   0.87396     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict = classifier3.predict(test_arrays2)\n",
    "print(metrics.classification_report(test_labels2, predict, digits = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "decimal-somalia",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict3 = metrics.classification_report(test_labels2, predict, digits = 5, output_dict = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-pharmaceutical",
   "metadata": {},
   "source": [
    "# 3. Results\n",
    "When the models are inspected, it seems that they have kind of understood the word 'good', since the most similar words to 'good' are 'decent' and 'great'. They also have kind of understood the word 'boring', since the most similar words to 'boring' are 'dull' and 'tedious'. Table 1 shows the three most similar words to the words 'good' and 'boring' for the four different settings. What stands out is that the third most similar word to 'good' for experimental setting 1 is 'bad'. This is strange since this has nothing to do with the word 'good'. The same goes for experimental setting 3, except that it is now the second most similar word to 'good'. Experimental setting 2 has the same similar words which is obvious since it uses the same model but a tuned classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "narrative-diary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Table 1: An overview of the three most similar words to 'boring' and 'good' for the different settings.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Similar to 'good'</th>\n",
       "      <th>Similar to 'boring'</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline setting</th>\n",
       "      <td>decent, great, nice</td>\n",
       "      <td>dull, tedious, pointless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Experiment 1</th>\n",
       "      <td>decent, great, bad</td>\n",
       "      <td>dull, pointless, tedious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Experiment 2</th>\n",
       "      <td>decent, great, nice</td>\n",
       "      <td>dull, tedious, pointless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Experiment 3</th>\n",
       "      <td>great, bad, decent</td>\n",
       "      <td>dull, bored, tedious</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Similar to 'good'       Similar to 'boring'\n",
       "Baseline setting  decent, great, nice  dull, tedious, pointless\n",
       "Experiment 1       decent, great, bad  dull, pointless, tedious\n",
       "Experiment 2      decent, great, nice  dull, tedious, pointless\n",
       "Experiment 3       great, bad, decent      dull, bored, tedious"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_three_good = model.most_similar('good')[0][0] + ', ' + model.most_similar('good')[1][0] + ', ' + model.most_similar('good')[2][0]\n",
    "model_three_boring = model.most_similar('boring')[0][0] + ', ' + model.most_similar('boring')[1][0] + ', ' + model.most_similar('boring')[2][0]\n",
    "\n",
    "model1_three_good = model1.most_similar('good')[0][0] + ', ' + model1.most_similar('good')[1][0] + ', ' + model1.most_similar('good')[2][0]\n",
    "model1_three_boring = model1.most_similar('boring')[0][0] + ', ' + model1.most_similar('boring')[1][0] + ', ' + model1.most_similar('boring')[2][0]\n",
    "\n",
    "model2_three_good = model2.most_similar('good')[0][0] + ', ' + model2.most_similar('good')[1][0] + ', ' + model2.most_similar('good')[2][0]\n",
    "model2_three_boring = model2.most_similar('boring')[0][0] + ', ' + model2.most_similar('boring')[1][0] + ', ' + model2.most_similar('boring')[2][0]\n",
    "\n",
    "\n",
    "conclus = {\"Similar to 'good'\":[model_three_good,\n",
    "                                model1_three_good,\n",
    "                                model_three_good,\n",
    "                                model2_three_good],\n",
    "          \"Similar to 'boring'\":[model_three_boring,\n",
    "                                 model1_three_boring,\n",
    "                                 model_three_boring,\n",
    "                                 model2_three_boring]}\n",
    "\n",
    "conclus_df = pd.DataFrame(conclus, index=['Baseline setting',\"Experiment 1\",'Experiment 2', 'Experiment 3'])\n",
    "\n",
    "print(\"\\nTable 1: An overview of the three most similar words to 'boring' and 'good' for the different settings.\")\n",
    "display(conclus_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "relative-ethnic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Table 2: The evaluation metrics of the baseline setting and the three experimental settings.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline setting</th>\n",
       "      <td>0.872004</td>\n",
       "      <td>0.87200</td>\n",
       "      <td>0.872000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Experiment 1</th>\n",
       "      <td>0.871082</td>\n",
       "      <td>0.87108</td>\n",
       "      <td>0.871080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Experiment 2</th>\n",
       "      <td>0.871803</td>\n",
       "      <td>0.87180</td>\n",
       "      <td>0.871800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Experiment 3</th>\n",
       "      <td>0.873973</td>\n",
       "      <td>0.87396</td>\n",
       "      <td>0.873959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Precision   Recall  F1-score\n",
       "Baseline setting   0.872004  0.87200  0.872000\n",
       "Experiment 1       0.871082  0.87108  0.871080\n",
       "Experiment 2       0.871803  0.87180  0.871800\n",
       "Experiment 3       0.873973  0.87396  0.873959"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conclus1 = {\"Precision\":[out_dict['weighted avg']['precision'],\n",
    "                         out_dict1['weighted avg']['precision'],\n",
    "                         out_dict2['weighted avg']['precision'],\n",
    "                         out_dict3['weighted avg']['precision']],\n",
    "            \"Recall\":[out_dict['weighted avg']['recall'],\n",
    "                      out_dict1['weighted avg']['recall'],\n",
    "                      out_dict2['weighted avg']['recall'],\n",
    "                      out_dict3['weighted avg']['recall']],\n",
    "            \"F1-score\":[out_dict['weighted avg']['f1-score'],\n",
    "                        out_dict1['weighted avg']['f1-score'],\n",
    "                        out_dict2['weighted avg']['f1-score'],\n",
    "                        out_dict3['weighted avg']['f1-score']]}\n",
    "\n",
    "conclus_df1 = pd.DataFrame(conclus1, index=['Baseline setting',\"Experiment 1\",'Experiment 2', 'Experiment 3'])\n",
    "\n",
    "print(\"\\nTable 2: The evaluation metrics of the baseline setting and the three experimental settings.\")\n",
    "display(conclus_df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-thesaurus",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "The goal of this project was to perform a sentiment classification task with Doc2Vec feature representation. Looking at Table 2, it can be concluded that the model with experimental setting 3 has the best precision, recall and F1-score. This model has a word embeddings model with a feature vector space of 300 instead of 100 (as in the baseline setting). However, all the evaluation metrics for the different settings are quite close to each other. So the three different settings chosen might not have been the best in improving the sentiment classification. In chapter 18 of the textbook 'Text Data Management and Analysis. A Practical Introduction to Information Retrieval and Text Mining', it is stated that among other features, the features character n-grams and word n-grams are important in sentiment classification. The experimental settings used in this project do not change these features. A change in these features might produce better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-accreditation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
